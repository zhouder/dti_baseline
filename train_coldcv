# -*- coding: utf-8 -*-
"""
MolTrans cold-start 5-fold cross-validation training script
- Reads /root/lanyun-tmp/<DATASET>/all.csv with columns: smiles, protein, label
- Renames columns to what MolTrans stream.BIN_Data_Encoder expects: SMILES, Target Sequence, Label
- Cold-start split by protein or drug (GroupKFold), 5 folds by default
- Inside each fold: sample a small validation set from the training pool (to reach overall train/val/test ≈ 0.7/0.1/0.2)
- Monitors AUPRC on validation set; keeps the best model for testing
- Logs per-epoch metrics to CSV and writes cv_summary.csv at the end
"""
from __future__ import annotations
import os, math, time, csv, argparse, copy
from pathlib import Path
from typing import Tuple, Dict, List

import numpy as np
import pandas as pd
import torch
from torch import nn, optim
from torch.utils.data import DataLoader
from torch.autograd import Variable

from sklearn.model_selection import GroupKFold
from sklearn.metrics import (
    roc_auc_score, average_precision_score, f1_score, accuracy_score,
    recall_score, matthews_corrcoef
)

# Import MolTrans components
from config import BIN_config_DBPE
from models import BIN_Interaction_Flat
from stream import BIN_Data_Encoder

def set_seed(seed: int = 42):
    import random
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

def compute_metrics(prob: np.ndarray, y_true: np.ndarray, thr: float = 0.5) -> Dict[str, float]:
    out: Dict[str, float] = {}
    pred = (prob >= thr).astype(np.int64)
    out["acc"] = float(accuracy_score(y_true, pred))
    out["sen"] = float(recall_score(y_true, pred))
    out["f1"]  = float(f1_score(y_true, pred))
    out["mcc"] = float(matthews_corrcoef(y_true, pred))
    try:    out["auroc"] = float(roc_auc_score(y_true, prob))
    except Exception: out["auroc"] = float("nan")
    try:    out["auprc"] = float(average_precision_score(y_true, prob))
    except Exception: out["auprc"] = float("nan")
    return out

def find_best_threshold(prob: np.ndarray, y_true: np.ndarray, grid: np.ndarray | None = None) -> float:
    if grid is None:
        grid = np.linspace(0.01, 0.99, 199)
    best_t, best_f1 = 0.5, -1.0
    for t in grid:
        pred = (prob >= t).astype(np.int64)
        try: f1 = f1_score(y_true, pred)
        except Exception: f1 = float("nan")
        if not np.isnan(f1) and f1 > best_f1:
            best_f1, best_t = float(f1), float(t)
    return float(best_t)

def sample_val_from_pool_by_groups(pool_idx: np.ndarray, groups: np.ndarray,
                                   val_frac_in_pool: float, seed: int) -> Tuple[np.ndarray, np.ndarray]:
    """In the pool (train+val candidate, excluding test), sample a subset of groups for val."""
    rng = np.random.default_rng(seed)
    pool_groups = groups[pool_idx]
    uniq = np.unique(pool_groups)
    rng.shuffle(uniq)
    n_val_groups = max(1, int(round(val_frac_in_pool * len(uniq))))
    val_groups = set(uniq[:n_val_groups])
    mask = np.array([g in val_groups for g in pool_groups], dtype=bool)
    va_idx = pool_idx[mask]
    tr_idx = pool_idx[~mask]
    return tr_idx, va_idx

@torch.no_grad()
def evaluate(model: nn.Module, loader: DataLoader, device: torch.device) -> Tuple[float, np.ndarray, np.ndarray]:
    model.eval()
    loss_f = nn.BCELoss()
    tot_loss = 0.0
    probs, labels = [], []
    for (d, p, d_mask, p_mask, label) in loader:
        score = model(d.long().to(device), p.long().to(device),
                      d_mask.long().to(device), p_mask.long().to(device))
        sigmoid = torch.nn.Sigmoid()
        logits = torch.squeeze(sigmoid(score))
        y = Variable(torch.from_numpy(np.array(label)).float()).to(device)
        loss = loss_f(logits, y)
        tot_loss += float(loss.detach().cpu())
        probs.append(logits.detach().float().cpu().numpy())
        labels.append(y.detach().cpu().numpy())
    prob = np.concatenate(probs, axis=0) if probs else np.zeros((0,), dtype=np.float32)
    lab  = np.concatenate(labels, axis=0) if labels else np.zeros((0,), dtype=np.float32)
    return tot_loss / max(1, len(loader)), prob, lab

def train_one_epoch(model: nn.Module, loader: DataLoader, device: torch.device,
                    optimizer: optim.Optimizer) -> float:
    model.train(True)
    loss_f = nn.BCELoss()
    tot_loss = 0.0
    n_batch = 0
    for (d, p, d_mask, p_mask, label) in loader:
        optimizer.zero_grad(set_to_none=True)
        score = model(d.long().to(device), p.long().to(device),
                      d_mask.long().to(device), p_mask.long().to(device))
        sigmoid = torch.nn.Sigmoid()
        logits = torch.squeeze(sigmoid(score))
        y = Variable(torch.from_numpy(np.array(label)).float()).to(device)
        loss = loss_f(logits, y)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)
        optimizer.step()
        tot_loss += float(loss.detach().cpu())
        n_batch += 1
    return tot_loss / max(1, n_batch)

def make_generators(df: pd.DataFrame, batch_size: int, num_workers: int) -> Tuple[DataLoader, DataLoader, DataLoader]:
    params = {'batch_size': batch_size, 'shuffle': True, 'num_workers': num_workers, 'drop_last': False}
    train_set = BIN_Data_Encoder(df.index.values, df.Label.values, df)
    return data.DataLoader(train_set, **params)

def parse_args():
    ap = argparse.ArgumentParser()
    ap.add_argument("--dataset", required=True, help="DAVIS / BindingDB / BioSNAP")
    ap.add_argument("--split-mode", choices=["cold-protein", "cold-drug"], default="cold-protein")
    ap.add_argument("--cv-folds", type=int, default=5)
    ap.add_argument("--overall-train", type=float, default=0.7)
    ap.add_argument("--overall-val", type=float, default=0.1)
    ap.add_argument("--epochs", type=int, default=13)
    ap.add_argument("--batch-size", type=int, default=16)
    ap.add_argument("--workers", type=int, default=4)
    ap.add_argument("--lr", type=float, default=1e-4)
    ap.add_argument("--seed", type=int, default=42)
    ap.add_argument("--out-dir", default="", help="Optional output root")
    ap.add_argument("--suffix", default="", help="Suffix in output dir name")
    return ap.parse_args()

if __name__ == "__main__":
    args = parse_args()
    set_seed(args.seed)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    if device.type == "cuda":
        try:
            print("GPU:", torch.cuda.get_device_name(0))
        except Exception:
            pass

    # ---- Load CSV from server ----
    ds_lower = args.dataset.lower()
    ds_cap   = {"bindingdb":"BindingDB", "davis":"DAVIS", "biosnap":"BioSNAP"}.get(ds_lower, args.dataset)
    data_root = Path("/root/lanyun-tmp")
    all_csv   = data_root / ds_cap / "all.csv"
    if not all_csv.exists():
        raise FileNotFoundError(f"not found: {all_csv}")
    df_all = pd.read_csv(all_csv)
    # unify column names expected by MolTrans stream.BIN_Data_Encoder
    rename_map = {}
    for k in list(df_all.columns):
        k_low = k.lower()
        if k_low == "smiles": rename_map[k] = "SMILES"
        if k_low in ("protein", "sequence", "target", "target sequence"): rename_map[k] = "Target Sequence"
        if k_low in ("label","y"): rename_map[k] = "Label"
    df_all = df_all.rename(columns=rename_map)
    if not {"SMILES", "Target Sequence", "Label"} <= set(df_all.columns):
        raise ValueError("CSV must contain columns: smiles, protein, label (any case)")
    df_all = df_all[["SMILES", "Target Sequence", "Label"]].dropna().reset_index(drop=True)

    N = len(df_all)
    print(f"[ALL] loaded: {N} rows from {all_csv}")
    groups_all = np.asarray(df_all["Target Sequence"] if args.split_mode == "cold-protein" else df_all["SMILES"])
    labels = df_all["Label"].values.astype(np.float32)

    # ---- Group KFold for cold-start ----
    gkf = GroupKFold(n_splits=args.cv_folds if args.cv_folds > 1 else 5)
    outer_splits = list(gkf.split(np.arange(N), groups=groups_all))
    overall_test = 1.0 / len(outer_splits)
    val_frac_in_pool = args.overall_val / (1.0 - overall_test)  # e.g. 0.1 / 0.8 = 0.125

    # ---- Output dir ----
    split_tag = "cold-protein" if args.split_mode == "cold-protein" else "cold-drug"
    base = args.out_dir if args.out_dir else f"/root/lanyun-tmp/moltrans-runs/{ds_cap}-{split_tag}"
    if args.suffix:
        base += (args.suffix if args.suffix.startswith("-") else f"-{args.suffix}")
    out_dir = Path(base)
    out_dir.mkdir(parents=True, exist_ok=True)
    print("[OUT]", out_dir)

    keys = ["auroc","auprc","f1","acc","sen","mcc"]
    test_metrics_all: List[Dict[str, float]] = []

    for fold_id, (_, te_idx) in enumerate(outer_splits, 1):
        te_idx = np.asarray(te_idx)
        pool_idx = np.setdiff1d(np.arange(N), te_idx)
        tr_idx, va_idx = sample_val_from_pool_by_groups(pool_idx, groups_all, val_frac_in_pool, seed=args.seed + fold_id)

        df_tr = df_all.iloc[tr_idx].copy()
        df_va = df_all.iloc[va_idx].copy()
        df_te = df_all.iloc[te_idx].copy()

        # build PyTorch DataLoaders via MolTrans stream
        params = {'batch_size': args.batch_size, 'shuffle': True, 'num_workers': args.workers, 'drop_last': False}
        dset_tr = BIN_Data_Encoder(df_tr.index.values, df_tr.Label.values, df_tr)
        dset_va = BIN_Data_Encoder(df_va.index.values, df_va.Label.values, df_va)
        dset_te = BIN_Data_Encoder(df_te.index.values, df_te.Label.values, df_te)
        loader_tr = DataLoader(dset_tr, **params)
        loader_va = DataLoader(dset_va, **params)
        loader_te = DataLoader(dset_te, **params)

        # model & optimizer
        config = BIN_config_DBPE()
        if args.epochs is not None and args.epochs > 0:
            config["train_epoch"] = args.epochs
        model = BIN_Interaction_Flat(**config)
        if torch.cuda.device_count() > 1:
            model = nn.DataParallel(model, dim=0)
        model = model.to(device)
        optimizer = optim.Adam(model.parameters(), lr=args.lr)

        # fold output
        fold_dir = out_dir / f"fold{fold_id}"
        fold_dir.mkdir(parents=True, exist_ok=True)
        csv_path  = fold_dir / "metrics.csv"
        with open(csv_path, "w", newline="") as f:
            w = csv.writer(f)
            w.writerow(["epoch","train_loss","val_loss","AUROC","AUPRC","F1","ACC","SEN","MCC","thr"])

        best_score = -1.0
        best_state = None
        best_thr = 0.5
        best_metrics = {}

        for ep in range(1, int(config["train_epoch"]) + 1):
            tr_loss = train_one_epoch(model, loader_tr, device, optimizer)
            va_loss, prob_va, y_va = evaluate(model, loader_va, device)
            thr_now = find_best_threshold(prob_va, y_va)
            m = compute_metrics(prob_va, y_va, thr=thr_now)
            with open(csv_path, "a", newline="") as f:
                w = csv.writer(f)
                w.writerow([ep, f"{tr_loss:.6f}", f"{va_loss:.6f}",
                            f"{m['auroc']:.6f}", f"{m['auprc']:.6f}", f"{m['f1']:.6f}",
                            f"{m['acc']:.6f}", f"{m['sen']:.6f}", f"{m['mcc']:.6f}", f"{thr_now:.6f}"])
            print(f"[{ds_cap}/fold{fold_id}] ep{ep:03d} | train_loss {tr_loss:.4f} | val_loss {va_loss:.4f} | "
                  f"AUROC {m['auroc']:.4f} | AUPRC {m['auprc']:.4f} | F1 {m['f1']:.4f} | thr {thr_now:.3f}")

            score = m["auprc"] if not math.isnan(m["auprc"]) else m["auroc"]
            if score > best_score:
                best_score = score
                best_state = copy.deepcopy(model.state_dict())
                best_thr = float(thr_now)
                best_metrics = dict(m)

        # test with best
        if best_state is not None:
            model.load_state_dict(best_state)
        te_loss, prob_te, y_te = evaluate(model, loader_te, device)
        te_m = compute_metrics(prob_te, y_te, thr=best_thr)
        print(f"[TEST/fold{fold_id}] thr={best_thr:.3f} | AUROC {te_m['auroc']:.4f} | AUPRC {te_m['auprc']:.4f} | "
              f"F1 {te_m['f1']:.4f} | ACC {te_m['acc']:.4f} | SEN {te_m['sen']:.4f} | MCC {te_m['mcc']:.4f} | loss {te_loss:.4f}")

        # write summary
        with open(fold_dir / "summary.csv", "w", newline="") as f:
            w = csv.writer(f)
            w.writerow(["VAL_BEST_EPOCH", int(np.argmax([0]+[])) if False else "n/a"])
            w.writerow(["VAL_BEST_THR",   f"{best_thr:.6f}"])
            for k, v in best_metrics.items():
                w.writerow([f"VAL_{k.upper()}", f"{v:.6f}"])
            for k, v in te_m.items():
                w.writerow([f"TEST_{k.upper()}", f"{v:.6f}"])

        test_metrics_all.append(te_m)

    # summary over folds
    mean = {k: float(np.mean([m[k] for m in test_metrics_all])) for k in keys}
    std  = {k: float(np.std ([m[k] for m in test_metrics_all])) for k in keys}
    with open(out_dir / "cv_summary.csv", "w", newline="") as f:
        w = csv.writer(f); w.writerow(["metric","mean","std"])
        for k in keys: w.writerow([k.upper(), f"{mean[k]:.6f}", f"{std[k]:.6f}"])
    print("[CV] " + " | ".join([f\"{k.upper()} {mean[k]:.4f}±{std[k]:.4f}\" for k in keys]))
